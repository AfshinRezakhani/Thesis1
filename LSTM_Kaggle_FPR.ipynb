{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDSDuodG0Q9OmRPlwM8pes",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfshinRezakhani/Thesis1/blob/main/LSTM_Kaggle_FPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwknG3jJ0qWT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/Kaggle.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['A'])\n",
        "y = df['A']\n",
        "\n",
        "# Scale the features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Separate classes\n",
        "X_class0 = X_scaled[y == 0]\n",
        "X_class1 = X_scaled[y == 1]\n",
        "\n",
        "latent_dim = 10\n",
        "\n",
        "# Define Generator Model\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(1, latent_dim)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dense(X_class0.shape[1], activation='tanh'),\n",
        "        RepeatVector(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define Discriminator Model\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(1, X_class0.shape[1])),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = tf.keras.Input(shape=(1, latent_dim))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = tf.keras.Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "\n",
        "# Train GAN\n",
        "for epoch in range(epochs):\n",
        "    noise = np.random.normal(0, 1, (batch_size, 1, latent_dim))\n",
        "    generated_data = generator.predict(noise)\n",
        "\n",
        "    idx = np.random.randint(0, X_class0.shape[0], batch_size)\n",
        "    real_data = X_class0[idx].reshape(batch_size, 1, -1)\n",
        "\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, 1, latent_dim))\n",
        "    g_loss = gan.train_on_batch(noise, real_labels)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch} | D Loss: {d_loss[0]:.4f} | G Loss: {g_loss:.4f}\")\n",
        "\n",
        "# Generate synthetic samples\n",
        "num_samples_needed = len(X_class1) - len(X_class0)\n",
        "noise = np.random.normal(0, 1, (num_samples_needed, 1, latent_dim))\n",
        "synthetic_data = generator.predict(noise).reshape(num_samples_needed, -1)\n",
        "\n",
        "synthetic_data = scaler.inverse_transform(synthetic_data)\n",
        "\n",
        "# Balance dataset\n",
        "X_balanced = np.vstack((X_scaled, synthetic_data))\n",
        "y_balanced = np.hstack((y, np.zeros(num_samples_needed)))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM Classifier\n",
        "classifier = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(1, X_train.shape[1])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, return_sequences=False),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Reshape data\n",
        "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
        "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
        "\n",
        "# Train classifier\n",
        "classifier.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (classifier.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f_score = f1_score(y_test, y_pred)\n",
        "\n",
        "# Compute confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "# Compute False Positive Rate (FPR) and False Negative Rate (FNR)\n",
        "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(f\"Balanced Dataset Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Balanced Dataset F1-Score: {f_score:.4f}\")\n",
        "print(f\"True Positives (TP): {tp}\")\n",
        "print(f\"True Negatives (TN): {tn}\")\n",
        "print(f\"False Positives (FP): {fp}\")\n",
        "print(f\"False Negatives (FN): {fn}\")\n",
        "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
        "print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
        "\n",
        "# Save balanced dataset\n",
        "balanced_df = pd.DataFrame(np.column_stack((y_balanced, X_balanced)), columns=['A'] + list(df.columns[1:]))\n",
        "balanced_df.to_csv(\"/content/Kaggle_balanced.csv\", index=False)\n",
        "\n",
        "print(\"Balanced dataset saved as Kaggle_balanced.csv\")\n"
      ]
    }
  ]
}